{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TfidfVectorizer_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyeonjungGit/NLP_Algorithm/blob/main/TfidfVectorizer_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuPqY8HTp_-J"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  # 텍스트 전처리\n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    '''\n",
        "    문제 1-1.\n",
        "    조건 1 : 소문자로의 변환과 특수문자 제거를 수행\n",
        "    조건 2 : 토큰화는 white space 단위로 수행\n",
        "    '''\n",
        "    from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "    for s in sequences :  # 한 문장씩 꺼내기\n",
        "      result.append(list(text_to_word_sequence(s)))  # 소문자로 변환 + 특수문자 제거(아포스트로피는 보존) + 토큰화\n",
        "\n",
        "    return result\n",
        "  \n",
        "\n",
        "  # 어휘 사전 구축\n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    '''\n",
        "    문제 1-2.\n",
        "    조건 1: 위에서 만든 preprocessing 함수를 이용하여 각 문장에 대해 토큰화를 수행합니다.\n",
        "    조건 2: 각각의 토큰을 정수 인덱싱 하기 위한 어휘 사전(self.word_dict)을 생성합니다.\n",
        "    주어진 코드에 있는 self.word_dict를 활용합니다.\n",
        "    '''\n",
        "    tokens = self.preprocessing(sequences)  # 중첩리스트가 반환\n",
        "\n",
        "    from collections import Counter\n",
        "    all_words_list = sum(tokens, [])  # 토큰들을 한 리스트에 모두 담기\n",
        "    vocab = Counter(all_words_list)  # 리스트에서 토큰들의 갯수 세아리기\n",
        "    vocab_size = len(vocab)    # 토큰 중에 어휘사전구축에 활용할 갯수 설정(여기서는 전체 토큰 활용)\n",
        "    vocab = vocab.most_common(vocab_size)\n",
        "\n",
        "    i = 0\n",
        "    for (word, frequency) in vocab :  # 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여\n",
        "      i += 1\n",
        "      self.word_dict[word] = i     # word_dict에 어휘 추가.\n",
        "    self.fit_checker = True\n",
        "\n",
        "\n",
        "  # 어휘 사전 활용 -> 정수 인덱싱\n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences)  # 중첩리스트가 반환\n",
        "    if self.fit_checker:\n",
        "      '''\n",
        "      문제 1-3.\n",
        "      조건 1: 어휘 사전(self.word_dict)에 없는 단어는 'oov'의 index로 변환합니다.\n",
        "      '''\n",
        "      for i in range(len(tokens)) :  # 한 문장을 불러옴\n",
        "        pre = []\n",
        "        for j in tokens[i] :  # 한 단어를 꺼낸다.\n",
        "          if j in self.word_dict.keys():  \n",
        "            pre.append(self.word_dict[j]) \n",
        "          else:    # 어휘 사전(self.word_dict)에 없는 단어는 'oov'의 index로 변환\n",
        "            pre.append(self.word_dict['oov'])\n",
        "        result.append(pre)\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "\n",
        "\n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)  # 이중리스트\n",
        "    '''\n",
        "    문제 2-1.\n",
        "    조건 1: IDF 행렬은 list 형태입니다.\n",
        "    조건 2: IDF 값은 아래 식을 이용해 구합니다.\n",
        "    - IDF = (특정단어 t가 등장한 문서의 수) 에 반비례하는 수\n",
        "    조건 3: 입력된 문장의 토큰화에는 문제 1에서 만든 Tokenizer를 사용합니다.\n",
        "    '''\n",
        "\n",
        "    from math import log\n",
        "\n",
        "    # 정수인덱싱된 문장을 어휘사전으로 만들기\n",
        "    vocab = list(set(sum(tokenized, [])))  # [1,2,3,4,...]\n",
        "    vocab.sort()  # 정렬\n",
        "\n",
        "    # 총 문서의 수\n",
        "    N = len(tokenized)\n",
        "\n",
        "    # 특정 단어 t가 등장한 문서의 수(IDF)\n",
        "    self.result_idf = []\n",
        "    for t in vocab :      # 어휘사전에서 한단어씩 꺼내기 e.g) 1 or 2 or 3....\n",
        "      df = 0\n",
        "      for one in tokenized:  # 전체에서 한 문장씩 꺼낸다. e.g) [1,2,3,4]\n",
        "        df += (t in one)   # 한 문장에 해당 단어가 들어가 있다. -> True | False   # 전체문서에서 특정단어 t가 얼만큼 등장?\n",
        "      \n",
        "      self.result_idf.append(log(N/(1+df)))  # 해당 단어(t)에 대한 idf 값을 리스트에 추가\n",
        "\n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      '''\n",
        "      문제 2-2.\n",
        "      조건1 : 입력 문장을 이용해 TF 행렬을 만드세요.\n",
        "      - tf(d, t) : 문장 d에 단어 t가 나타난 횟수\n",
        "      조건2 : 문제 2-1( fit())에서 만든 IDF 행렬과 아래 식을 이용해 TF-IDF 행렬을 만드세요\n",
        "      - tf-idf(d,t) = tf(d,t) * idf(d,t)\n",
        "      '''\n",
        "      # 정수인덱싱된 문장을 어휘사전으로 만들기\n",
        "      vocab = list(set(sum(tokenized, [])))\n",
        "      vocab.sort()  # 정렬    \n",
        "\n",
        "      # 총 문서의 수\n",
        "      N = len(tokenized)\n",
        "      \n",
        "      # 한 문서안에 특정단어 t가 몇번 들어가는지 (TF)\n",
        "      self.result_tf = []\n",
        "      for d in tokenized:  # 문장 한개를 꺼낸다.\n",
        "        self.result_tf.append([])\n",
        "        for t in vocab:  # 어휘사전에서 단어 한개 꺼낸다.\n",
        "          self.result_tf[-1].append(d.count(t))  # 중첩리스트\n",
        "\n",
        "      # tfidf 구하기\n",
        "      self.tfidf_matrix = []\n",
        "      for tf in self.result_tf :  # TF행렬 하나씩 꺼내기 (문서 1개씩 꺼내기)\n",
        "        matrix = []\n",
        "        for i in range(len(vocab)):\n",
        "          matrix.append(tf[i]*self.result_idf[i])\n",
        "        self.tfidf_matrix.append(matrix)\n",
        "      return self.tfidf_matrix\n",
        "\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)  # 중첩리스트"
      ],
      "metadata": {
        "id": "h4GK4fo3clj5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}